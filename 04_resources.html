
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>External Meta-Analytic Resources &#8212; NiMARE: Neuroimaging Meta-Analysis Research Environment</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="_static/nimare_favicon.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Coordinate-Based Meta-Analysis" href="05_cbma.html" />
    <link rel="prev" title="Download the Data" href="03_download_data.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/nimare_overview.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">NiMARE: Neuroimaging Meta-Analysis Research Environment</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="00_abstract.html">
   Abstract
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  About NiMARE
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_overview.html">
   NiMARE Overview
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Meta-Analytic Databases and Resources
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="03_download_data.html">
   Download the Data
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   External Meta-Analytic Resources
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Meta-Analyses in NiMARE
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="05_cbma.html">
   Coordinate-Based Meta-Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_ibma.html">
   Image-Based Meta-Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_correction.html">
   Multiple Comparisons Correction
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Other Meta-Analytic Approaches
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="08_about_derivative_analyses.html">
   Derivative Analyses
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_subtraction.html">
   Meta-Analytic Subtraction Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_macm.html">
   Meta-Analytic Coactivation Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_annotation.html">
   Automated Annotation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12_decoding.html">
   Meta-Analytic Functional Decoding
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Concluding Thoughts
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="13_future_directions.html">
   Future Directions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14_summary.html">
   Summary
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15_acknowledgements.html">
   Acknowledgements
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="16_bibliography.html">
   References
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="appendices/brainmap_decoding.html">
   Appendix I: BrainMap Discrete Decoding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="appendices/neurosynth_decoding.html">
   Appendix II: Neurosynth Discrete Decoding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="17_build_information.html">
   Build Information
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="_sources/04_resources.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/04_resources.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/NBCLab/nimare-paper"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/NBCLab/nimare-paper/issues/new?title=Issue%20on%20page%20%2F04_resources.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://binder.conp.cloud/v2/gh/NBCLab/nimare-paper/master?urlpath=tree/content/04_resources.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#brainmap">
   BrainMap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neurosynth">
   Neurosynth
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neuroquery">
   NeuroQuery
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neurovault">
   NeuroVault
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="external-meta-analytic-resources">
<h1>External Meta-Analytic Resources<a class="headerlink" href="#external-meta-analytic-resources" title="Permalink to this headline">¶</a></h1>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First, import the necessary modules and functions</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="kn">from</span> <span class="nn">repo2data.repo2data</span> <span class="kn">import</span> <span class="n">Repo2Data</span>

<span class="c1"># Install the data if running locally, or points to cached data if running on neurolibre</span>
<span class="n">DATA_REQ_FILE</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;../binder/data_requirement.json&quot;</span><span class="p">)</span>
<span class="n">repo2data</span> <span class="o">=</span> <span class="n">Repo2Data</span><span class="p">(</span><span class="n">DATA_REQ_FILE</span><span class="p">)</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="n">repo2data</span><span class="o">.</span><span class="n">install</span><span class="p">()</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;data&quot;</span><span class="p">)</span>

<span class="c1"># Set an output directory for any files generated during the book building process</span>
<span class="n">out_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;../outputs/&quot;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>---- repo2data starting ----
/home/data/nbc/misc-projects/Salo_NiMARE/conda_env/lib/python3.7/site-packages/repo2data
Config from file :
/home/data/nbc/misc-projects/Salo_NiMARE/binder/data_requirement.json
Destination:
./../data/nimare-paper

Info : ./../data/nimare-paper already downloaded
</pre></div>
</div>
</div>
</div>
<p>Large-scale meta-analytic databases have made systematic meta-analyses of the neuroimaging literature possible.
These databases combine results from neuroimaging studies, whether represented as coordinates of peak activations or unthresholded statistical images, with important study metadata, such as information about the samples acquired, stimuli used, analyses performed, and mental constructs putatively manipulated.
The two most popular coordinate-based meta-analytic databases are <a class="reference external" href="http://www.brainmap.org">BrainMap</a> and <a class="reference external" href="http://neurosynth.org">Neurosynth</a>, while the most popular image-based database is <a class="reference external" href="https://neurovault.org">NeuroVault</a>.</p>
<p>The studies archived in these databases may be either manually or automatically annotated—often with reference to a formal ontology or controlled vocabulary.
Ontologies for cognitive neuroscience define what mental states or processes are postulated to be manipulated or measured in experiments, and may also include details of said experiments (e.g.,the cognitive tasks employed), relationships between concepts (e.g., verbal working memory is a kind of working memory), and various other metadata that can be standardized and represented in a machine-readable form <span id="id1">[<a class="reference internal" href="16_bibliography.html#id63">Poldrack and Yarkoni, 2016</a>, <a class="reference internal" href="16_bibliography.html#id81">Poldrack, 2010</a>, <a class="reference internal" href="16_bibliography.html#id58">Turner and Laird, 2012</a>]</span>.
Some of these ontologies are very well-defined, such as expert-generated taxonomies designed specifically to describe only certain aspects of experiments and the relationships between elements within the taxonomy, while others are more loosely defined, in some cases simply building a vocabulary based on which terms are commonly used in cognitive neuroscience articles.</p>
<div class="section" id="brainmap">
<span id="content-resources-brainmap"></span><h2>BrainMap<a class="headerlink" href="#brainmap" title="Permalink to this headline">¶</a></h2>
<p><strong>BrainMap</strong> <span id="id2">[<a class="reference internal" href="16_bibliography.html#id51">Fox <em>et al.</em>, 2005</a>, <a class="reference internal" href="16_bibliography.html#id50">Fox and Lancaster, 2002</a>, <a class="reference internal" href="16_bibliography.html#id33">Laird <em>et al.</em>, 2005</a>]</span> relies on expert annotators to label individual comparisons within studies according to its internally developed ontology, the BrainMap Taxonomy <span id="id3">[<a class="reference internal" href="16_bibliography.html#id51">Fox <em>et al.</em>, 2005</a>]</span>.
While this approach is likely to be less noisy than an automated annotation method using article text or imaging results to predict content, it is also subject to a number of limitations.
First, there are simply not enough annotators to keep up with the ever-expanding literature.
Second, any development of the underlying ontology has the potential to leave the database outdated.
For example, if a new label is added to the BrainMap Taxonomy, then each study in the full BrainMap database needs to be evaluated for that label before that label can be properly integrated into the database.
Finally, a manually annotated database like BrainMap will be biased by which subdomains within the literature are annotated.
While outside contributors can add and annotate studies to the database, the main source of annotations has been researchers associated with the BrainMap project.</p>
<p>While BrainMap is a semi-closed resource (i.e., a collaboration agreement is required to access the full database), registered users may search the database using the Sleuth search tool, in order to collect samples for meta-analyses.
Sleuth can export these study collections as text files with coordinates.
NiMARE provides a function to import data from Sleuth text files into the NiMARE Dataset format.</p>
<p>The function <a class="reference external" href="https://nimare.readthedocs.io/en/0.0.11/generated/nimare.io.convert_sleuth_to_dataset.html#nimare.io.convert_sleuth_to_dataset" title="(in NiMARE v0.0.11+0.g7e094f2.dirty)"><code class="xref py py-func docutils literal notranslate"><span class="pre">convert_sleuth_to_dataset()</span></code></a> can be used to convert text files exported from Sleuth into NiMARE <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s.
Here, we convert two files from a previous publication by NiMARE contributors <span id="id4">[<a class="reference internal" href="16_bibliography.html#id96">Yanes <em>et al.</em>, 2018</a>]</span> into two separate <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nimare</span> <span class="kn">import</span> <span class="n">io</span>

<span class="n">sleuth_dset1</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">convert_sleuth_to_dataset</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;contrast-CannabisMinusControl_space-talairach_sleuth.txt&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">sleuth_dset2</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">convert_sleuth_to_dataset</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;contrast-ControlMinusCannabis_space-talairach_sleuth.txt&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sleuth_dset1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sleuth_dset2</span><span class="p">)</span>

<span class="c1"># Save the Datasets to files for future use</span>
<span class="n">sleuth_dset1</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="s2">&quot;sleuth_dset1.pkl.gz&quot;</span><span class="p">))</span>
<span class="n">sleuth_dset2</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="s2">&quot;sleuth_dset2.pkl.gz&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataset(41 experiments, space=&#39;ale_2mm&#39;)
Dataset(41 experiments, space=&#39;ale_2mm&#39;)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="neurosynth">
<h2>Neurosynth<a class="headerlink" href="#neurosynth" title="Permalink to this headline">¶</a></h2>
<p><strong>Neurosynth</strong> <span id="id5">[<a class="reference internal" href="16_bibliography.html#id26">Yarkoni <em>et al.</em>, 2011</a>]</span> uses a combination of web scraping and text mining to automatically harvest neuroimaging studies from the literature and to annotate them based on term frequency within article abstracts.
As a consequence of its relatively crude automated approach, Neurosynth has its own set of limitations.
First, Neurosynth is unable to delineate individual comparisons within studies, and consequently uses the entire paper as its unit of measurement, unlike BrainMap.
This risks conflating directly contrasted comparisons (e.g., A&gt;B and B&gt;A), as well as comparisons which have no relation to one another.
Second, coordinate extraction and annotation are noisy.
Third, annotations automatically performed by Neurosynth are also subject to error, although the reasons behind this are more nuanced and will be discussed later in this paper.
Given Neurosynth’s limitations, we recommend that it be used for casual, exploratory meta-analyses rather than for publication-quality analyses.
Nevertheless, while individual meta-analyses should not be published from Neurosynth, many derivative analyses have been performed and published (e.g., <span id="id6">[<a class="reference internal" href="16_bibliography.html#id85">Chang <em>et al.</em>, 2013</a>, <a class="reference internal" href="16_bibliography.html#id43">de la Vega <em>et al.</em>, 2016</a>, <a class="reference internal" href="16_bibliography.html#id39">de la Vega <em>et al.</em>, 2018</a>, <a class="reference internal" href="16_bibliography.html#id4">Poldrack <em>et al.</em>, 2012</a>]</span>).
As evidence of its utility, Neurosynth has been used to define <em>a priori</em> regions of interest (e.g., <span id="id7">[<a class="reference internal" href="16_bibliography.html#id70">Josipovic, 2014</a>, <a class="reference internal" href="16_bibliography.html#id25">Zeidman <em>et al.</em>, 2012</a>, <a class="reference internal" href="16_bibliography.html#id12">Wager <em>et al.</em>, 2013</a>]</span>) or perform meta-analytic functional decoding (e.g., <span id="id8">[<a class="reference internal" href="16_bibliography.html#id19">Chen <em>et al.</em>, 2018</a>, <a class="reference internal" href="16_bibliography.html#id69">Pantelis <em>et al.</em>, 2015</a>, <a class="reference internal" href="16_bibliography.html#id52">Tambini <em>et al.</em>, 2017</a>]</span>) in many first-order (rather than meta-analytic) fMRI studies.</p>
<p>Here, we show code that would download the Neurosynth database from where it is stored (<a class="reference external" href="https://github.com/neurosynth/neurosynth-data">https://github.com/neurosynth/neurosynth-data</a>) and convert it to a NiMARE <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> using <a class="reference external" href="https://nimare.readthedocs.io/en/0.0.11/generated/nimare.extract.fetch_neurosynth.html#nimare.extract.fetch_neurosynth" title="(in NiMARE v0.0.11+0.g7e094f2.dirty)"><code class="xref py py-func docutils literal notranslate"><span class="pre">fetch_neurosynth()</span></code></a>, for the first step, and <a class="reference external" href="https://nimare.readthedocs.io/en/0.0.11/generated/nimare.io.convert_neurosynth_to_dataset.html#nimare.io.convert_neurosynth_to_dataset" title="(in NiMARE v0.0.11+0.g7e094f2.dirty)"><code class="xref py py-func docutils literal notranslate"><span class="pre">convert_neurosynth_to_dataset()</span></code></a>, for the second.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nimare</span> <span class="kn">import</span> <span class="n">extract</span>

<span class="c1"># Download the desired version of Neurosynth from GitHub.</span>
<span class="n">files</span> <span class="o">=</span> <span class="n">extract</span><span class="o">.</span><span class="n">fetch_neurosynth</span><span class="p">(</span>
    <span class="n">data_dir</span><span class="o">=</span><span class="n">data_path</span><span class="p">,</span>
    <span class="n">version</span><span class="o">=</span><span class="s2">&quot;7&quot;</span><span class="p">,</span>
    <span class="n">source</span><span class="o">=</span><span class="s2">&quot;abstract&quot;</span><span class="p">,</span>
    <span class="n">vocab</span><span class="o">=</span><span class="s2">&quot;terms&quot;</span><span class="p">,</span>
    <span class="n">overwrite</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">files</span><span class="p">)</span>
<span class="n">neurosynth_db</span> <span class="o">=</span> <span class="n">files</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:nimare.extract.utils:Dataset found in ./../data/nimare-paper/data/neurosynth
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:nimare.extract.extract:Searching for any feature files matching the following criteria: [(&#39;source-abstract&#39;, &#39;vocab-terms&#39;, &#39;data-neurosynth&#39;, &#39;version-7&#39;)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data-neurosynth_version-7_coordinates.tsv.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>File exists and overwrite is False. Skipping.
Downloading data-neurosynth_version-7_metadata.tsv.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>File exists and overwrite is False. Skipping.
Downloading data-neurosynth_version-7_vocab-terms_source-abstract_type-tfidf_features.npz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>File exists and overwrite is False. Skipping.
Downloading data-neurosynth_version-7_vocab-terms_vocabulary.txt
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>File exists and overwrite is False. Skipping.
[{&#39;coordinates&#39;: &#39;/home/data/nbc/misc-projects/Salo_NiMARE/data/nimare-paper/data/neurosynth/data-neurosynth_version-7_coordinates.tsv.gz&#39;,
  &#39;features&#39;: [{&#39;features&#39;: &#39;/home/data/nbc/misc-projects/Salo_NiMARE/data/nimare-paper/data/neurosynth/data-neurosynth_version-7_vocab-terms_source-abstract_type-tfidf_features.npz&#39;,
                &#39;vocabulary&#39;: &#39;/home/data/nbc/misc-projects/Salo_NiMARE/data/nimare-paper/data/neurosynth/data-neurosynth_version-7_vocab-terms_vocabulary.txt&#39;}],
  &#39;metadata&#39;: &#39;/home/data/nbc/misc-projects/Salo_NiMARE/data/nimare-paper/data/neurosynth/data-neurosynth_version-7_metadata.tsv.gz&#39;}]
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Converting the large Neurosynth and NeuroQuery datasets to NiMARE <a class="reference external" href="https://nimare.readthedocs.io/en/0.0.11/generated/nimare.dataset.Dataset.html#nimare.dataset.Dataset" title="(in NiMARE v0.0.11+0.g7e094f2.dirty)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> objects can be a very memory-intensive process.
For the sake of this book, we show how to perform the conversions below, but actually load and use pre-converted <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert the files to a Dataset.</span>
<span class="c1"># This may take a while (~10 minutes)</span>
<span class="n">neurosynth_dset</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">convert_neurosynth_to_dataset</span><span class="p">(</span>
    <span class="n">coordinates_file</span><span class="o">=</span><span class="n">neurosynth_db</span><span class="p">[</span><span class="s2">&quot;coordinates&quot;</span><span class="p">],</span>
    <span class="n">metadata_file</span><span class="o">=</span><span class="n">neurosynth_db</span><span class="p">[</span><span class="s2">&quot;metadata&quot;</span><span class="p">],</span>
    <span class="n">annotations_files</span><span class="o">=</span><span class="n">neurosynth_db</span><span class="p">[</span><span class="s2">&quot;features&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">neurosynth_dset</span><span class="p">)</span>

<span class="c1"># Save the Dataset for later use.</span>
<span class="n">neurosynth_dset</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="s2">&quot;neurosynth_dataset.pkl.gz&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>Here, we load a pre-generated version of the Neurosynth <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nimare</span> <span class="kn">import</span> <span class="n">dataset</span>

<span class="n">neurosynth_dset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;neurosynth_dataset.pkl.gz&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">neurosynth_dset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataset(14371 experiments, space=&#39;mni152_2mm&#39;)
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Many of the methods in NiMARE can be very time-consuming or memory-intensive.
Therefore, for the sake of ensuring that the analyses in this article may be reproduced by as many people as possible, we will use a reduced version of the Neurosynth <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>, only containing the first 500 studies, for those methods which may not run easily on the full database.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">neurosynth_dset_first_500</span> <span class="o">=</span> <span class="n">neurosynth_dset</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">neurosynth_dset</span><span class="o">.</span><span class="n">ids</span><span class="p">[:</span><span class="mi">500</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">neurosynth_dset</span><span class="p">)</span>

<span class="c1"># Save this Dataset for later use.</span>
<span class="n">neurosynth_dset_first_500</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="s2">&quot;neurosynth_dataset_first500.pkl.gz&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataset(14371 experiments, space=&#39;mni152_2mm&#39;)
</pre></div>
</div>
</div>
</div>
<p>In addition to a large corpus of coordinates, Neurosynth provides term frequencies derived from article abstracts that can be used as annotations.</p>
<p>One additional benefit to Neurosynth is that it has made available the coordinates for a large number of studies for which the study abstracts are also readily available.
This has made the Neurosynth database a common resource upon which to build other automated ontologies.
Data-driven ontologies which have been developed using the Neurosynth database include the generalized correspondence latent Dirichlet allocation (GCLDA) <span id="id9">[<a class="reference internal" href="16_bibliography.html#id83">Rubin <em>et al.</em>, 2017</a>]</span> topic model and Deep Boltzmann machines <span id="id10">[<a class="reference internal" href="16_bibliography.html#id77">Monti <em>et al.</em>, 2016</a>]</span>.</p>
</div>
<div class="section" id="neuroquery">
<h2>NeuroQuery<a class="headerlink" href="#neuroquery" title="Permalink to this headline">¶</a></h2>
<p>A related resource is <strong>NeuroQuery</strong> <span id="id11">[<a class="reference internal" href="16_bibliography.html#id38">Dockès <em>et al.</em>, 2020</a>]</span>.
NeuroQuery is an online service for large-scale predictive meta-analysis.
Unlike Neurosynth, which performs statistical inference and produces statistical maps, NeuroQuery is a supervised learning model and produces a prediction of the brain areas most likely to contain activations.
These maps predict locations where studies investigating a given area (determined by the text prompt) are likely to produce activations, but they cannot be used in the same manner as statistical maps from a standard coordinate-based meta-analysis.
In addition to this predictive meta-analytic tool, NeuroQuery also provides a new database of coordinates, text annotations, and metadata via an automated extraction approach that improves on Neurosynth’s original methods.</p>
<p>While NiMARE does not currently include an interface to NeuroQuery’s predictive meta-analytic method, there are functions for downloading the NeuroQuery database and converting it to NiMARE format, much like Neurosynth.
The functions for downloading the NeuroQuery database and converting it to a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> are <a class="reference external" href="https://nimare.readthedocs.io/en/0.0.11/generated/nimare.extract.fetch_neuroquery.html#nimare.extract.fetch_neuroquery" title="(in NiMARE v0.0.11+0.g7e094f2.dirty)"><code class="xref py py-func docutils literal notranslate"><span class="pre">fetch_neuroquery()</span></code></a> and <a class="reference external" href="https://nimare.readthedocs.io/en/0.0.11/generated/nimare.io.convert_neurosynth_to_dataset.html#nimare.io.convert_neurosynth_to_dataset" title="(in NiMARE v0.0.11+0.g7e094f2.dirty)"><code class="xref py py-func docutils literal notranslate"><span class="pre">convert_neurosynth_to_dataset()</span></code></a>, respectively.
We are able to use the same function for converting the database to a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> for NeuroQuery as Neurosynth because both databases store their data in the same structure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download the desired version of NeuroQuery from GitHub.</span>
<span class="n">files</span> <span class="o">=</span> <span class="n">extract</span><span class="o">.</span><span class="n">fetch_neuroquery</span><span class="p">(</span>
    <span class="n">data_dir</span><span class="o">=</span><span class="n">data_path</span><span class="p">,</span>
    <span class="n">version</span><span class="o">=</span><span class="s2">&quot;1&quot;</span><span class="p">,</span>
    <span class="n">source</span><span class="o">=</span><span class="s2">&quot;combined&quot;</span><span class="p">,</span>
    <span class="n">vocab</span><span class="o">=</span><span class="s2">&quot;neuroquery6308&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;tfidf&quot;</span><span class="p">,</span>
    <span class="n">overwrite</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">files</span><span class="p">)</span>
<span class="n">neuroquery_db</span> <span class="o">=</span> <span class="n">files</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:nimare.extract.utils:Dataset found in ./../data/nimare-paper/data/neuroquery
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:nimare.extract.extract:Searching for any feature files matching the following criteria: [(&#39;source-combined&#39;, &#39;vocab-neuroquery6308&#39;, &#39;type-tfidf&#39;, &#39;data-neuroquery&#39;, &#39;version-1&#39;)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data-neuroquery_version-1_coordinates.tsv.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>File exists and overwrite is False. Skipping.
Downloading data-neuroquery_version-1_metadata.tsv.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>File exists and overwrite is False. Skipping.
Downloading data-neuroquery_version-1_vocab-neuroquery6308_source-combined_type-tfidf_features.npz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>File exists and overwrite is False. Skipping.
Downloading data-neuroquery_version-1_vocab-neuroquery6308_vocabulary.txt
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>File exists and overwrite is False. Skipping.
[{&#39;coordinates&#39;: &#39;/home/data/nbc/misc-projects/Salo_NiMARE/data/nimare-paper/data/neuroquery/data-neuroquery_version-1_coordinates.tsv.gz&#39;,
  &#39;features&#39;: [{&#39;features&#39;: &#39;/home/data/nbc/misc-projects/Salo_NiMARE/data/nimare-paper/data/neuroquery/data-neuroquery_version-1_vocab-neuroquery6308_source-combined_type-tfidf_features.npz&#39;,
                &#39;vocabulary&#39;: &#39;/home/data/nbc/misc-projects/Salo_NiMARE/data/nimare-paper/data/neuroquery/data-neuroquery_version-1_vocab-neuroquery6308_vocabulary.txt&#39;}],
  &#39;metadata&#39;: &#39;/home/data/nbc/misc-projects/Salo_NiMARE/data/nimare-paper/data/neuroquery/data-neuroquery_version-1_metadata.tsv.gz&#39;}]
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert the files to a Dataset.</span>
<span class="c1"># This may take a while (~10 minutes)</span>
<span class="n">neuroquery_dset</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">convert_neurosynth_to_dataset</span><span class="p">(</span>
    <span class="n">coordinates_file</span><span class="o">=</span><span class="n">neuroquery_db</span><span class="p">[</span><span class="s2">&quot;coordinates&quot;</span><span class="p">],</span>
    <span class="n">metadata_file</span><span class="o">=</span><span class="n">neuroquery_db</span><span class="p">[</span><span class="s2">&quot;metadata&quot;</span><span class="p">],</span>
    <span class="n">annotations_files</span><span class="o">=</span><span class="n">neuroquery_db</span><span class="p">[</span><span class="s2">&quot;features&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">neuroquery_dset</span><span class="p">)</span>

<span class="c1"># Save the Dataset for later use.</span>
<span class="n">neuroquery_dset</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="s2">&quot;neuroquery_dataset.pkl.gz&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>Here, we load a pre-generated version of the NeuroQuery <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">neuroquery_dset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;neuroquery_dataset.pkl.gz&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">neuroquery_dset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataset(13459 experiments, space=&#39;mni152_2mm&#39;)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="neurovault">
<h2>NeuroVault<a class="headerlink" href="#neurovault" title="Permalink to this headline">¶</a></h2>
<p><strong>NeuroVault</strong> <span id="id12">[<a class="reference internal" href="16_bibliography.html#id10">Gorgolewski <em>et al.</em>, 2015</a>]</span> is a public repository of user-uploaded, whole-brain, unthresholded brain maps.
Users may associate their image collections with publications, and can annotate individual maps with labels from the Cognitive Atlas, which is the ontology of choice for NeuroVault.
NiMARE includes a function, <a class="reference external" href="https://nimare.readthedocs.io/en/0.0.11/generated/nimare.io.convert_neurovault_to_dataset.html#nimare.io.convert_neurovault_to_dataset" title="(in NiMARE v0.0.11+0.g7e094f2.dirty)"><code class="xref py py-func docutils literal notranslate"><span class="pre">convert_neurovault_to_dataset()</span></code></a>, with which users can search for images in NeuroVault, download those images, and convert them into a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> object.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="03_download_data.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Download the Data</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="05_cbma.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Coordinate-Based Meta-Analysis</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Taylor Salo, Tal Yarkoni, Thomas E. Nichols, Jean-Baptiste Poline, Murat Bilgel, Katherine L. Bottenhorn, Dorota Jarecka, James D. Kent, Adam Kimbler, Dylan M. Nielson, Kendra M. Oudyk, Julio A. Peraza, Alexandre Pérez, Puck C. Reeders, Julio A. Yanes, and Angela R. Laird<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>